{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AABknAuK_8UD",
        "outputId": "48d13975-e853-4606-f630-60f256667e28"
      },
      "source": [
        "!pip install wandb\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting appdirs>=1.4.3\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (63.4.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.17.0-py2.py3-none-any.whl (189 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.1/189.1 KB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.4)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.27.1)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=b47b8bd11b4a40734286bd717cb4bae505879d0fc3001f6feb5723188ee3f7cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, appdirs, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.17.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "evs-qvyd8Lll"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vuMLV7jAhpo"
      },
      "source": [
        "import numpy as np\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import wandb\n",
        "import time\n",
        "import wandb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation functions and their derivatives\n"
      ],
      "metadata": {
        "id": "ncNXUe_58jb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1 + np.exp(-(z)))\n",
        "\n",
        "def tanh(z):\n",
        "    return np.tanh(z)\n",
        "\n",
        "def sin(z):\n",
        "    return np.sin(z)\n",
        "\n",
        "def relu(z):\n",
        "    return (z>0)*(z) + ((z<0)*(z)*0.01)\n",
        "  \n",
        "def softmax(Z):\n",
        "    return np.exp(Z) / np.sum(np.exp(Z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    return  (1.0 / (1 + np.exp(-(z))))*(1 -  1.0 / (1 + np.exp(-(z))))\n",
        "\n",
        "def tanh_derivative(z):\n",
        "    return 1 - np.tanh(z) ** 2\n",
        "\n",
        "def relu_derivative(z):\n",
        "    return (z>0)*np.ones(z.shape) + (z<0)*(0.01*np.ones(z.shape) )"
      ],
      "metadata": {
        "id": "6kxUD5FE8O2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Rcbw2UiAs_M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "2faf58ca-d629-4c44-85e4-7543a99aafa4"
      },
      "source": [
        "\n",
        "\n",
        "class Feed_Forward_NN __init__(\n",
        "        self, \n",
        "        hidden_layers, \n",
        "        hidden_neurons, \n",
        "        Xtrain_ini, \n",
        "        ytrain_ini  \n",
        "        train_n, \n",
        "        Xval_ini \n",
        "        yval_ini \n",
        "        val_n,\n",
        "        Xtest_ini \n",
        "        ytest_ini \n",
        "        N_test,        \n",
        "        optimizer,\n",
        "        batch_size,\n",
        "        weight_decay,\n",
        "        learning_rate,\n",
        "        maxNo_epochh,\n",
        "        activation,\n",
        "        initializer,\n",
        "        loss\n",
        "\n",
        "        '''\n",
        "         Initializing the Feed_Forward_NN class.\n",
        "        '''\n",
        "        \n",
        "        self.classes = np.max(ytrain_ini) + 1  # number of classes\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.hidden_neurons = hidden_neurons\n",
        "        self.op = self.classes  #\n",
        "        self.img_height = Xtrain_ini.shape[1]\n",
        "        self.img_width = Xtrain_ini.shape[2]\n",
        "        self.size_imgflat = self.img_height * self.img_width\n",
        "        self.layers = ([self.size_imgflat]+ hidden_layers * [hidden_neurons]+ [self.op])\n",
        "        self.train_n = train_n\n",
        "        self.val_n = val_n\n",
        "        self.N_test = N_test\n",
        "        \n",
        "\n",
        "\n",
        "        self.X_train = np.transpose(Xtrain_ini.reshape(Xtrain_ini.shape[0], Xtrain_ini.shape[1] * Xtrain_ini.shape[2]))  \n",
        "        self.X_test = np.transpose(Xtest_ini.reshape(Xtest_ini.shape[0], Xtest_ini.shape[1] * Xtest_ini.shape[2]))\n",
        "        self.X_val = np.transpose(Xval_ini.reshape(Xval_ini.shape[0], Xval_ini.shape[1] * Xval_ini.shape[2]))\n",
        "    \n",
        "\n",
        "        self.X_train = self.X_train / 255\n",
        "        self.X_test = self.X_test / 255\n",
        "        self.X_val = self.X_val / 255\n",
        "        \n",
        "        self.y_train = self.one_hot_encode(ytrain_ini)\n",
        "        self.y_val = self.one_hot_encode(yval_ini)\n",
        "        self.y_test = self.one_hot_encode(ytest_ini)\n",
        "       \n",
        "#Activation Functions\n",
        "\n",
        "        self.activation_fns = {\"SIGMOID\": sigmoid, \"TANH\": tanh, \"RELU\": relu}\n",
        "        self.dv_activation_fns = {\n",
        "            \"SIGMOID\": sigmoid_derivative,\n",
        "            \"TANH\": tanh_derivative,\n",
        "            \"RELU\": relu_derivative,\n",
        "        }\n",
        "\n",
        "#Initializers\n",
        "\n",
        "        self.Initializers = {\n",
        "            \"XAVIER\": self.Xavier,\n",
        "            \"RANDOM\": self.Random,\n",
        "            \"HE\": self.He\n",
        "        }\n",
        "\n",
        "#Optimizers\n",
        "\n",
        "        self.Optimizers = {\n",
        "            \"SGD\": self.sgd_mb,\n",
        "            \"MGD\": self.mgd,\n",
        "            \"Ng: self.Ng,\n",
        "            \"RMSPROP\": self.rmsProp,\n",
        "            \"ADAM\": self.adam,\n",
        "            \"NADAM\": self.nadam,\n",
        "        }\n",
        "        \n",
        "        self.activation = self.activation_fns[activation] \n",
        "        self.der_activation = self.dv_activation_fns[activation]\n",
        "        self.optimizer = self.Optimizers[optimizer]\n",
        "        self.initializer = self.Initializers[initializer]\n",
        "        self.loss_function = loss\n",
        "        self.maxNo_epoch = maxNo_epoch\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        self.weights, self.biases = self.initialize_NN(self.layers)\n",
        "\n",
        "\n",
        "    def one_hot_encode(self, ytrain_ini):\n",
        "        y_data= np.zeros((self.classes, ytrain_ini.shape[0]))\n",
        "        for i in range(ytrain_ini.shape[0]):\n",
        "            value = ytrain_ini[i]\n",
        "            y_data[int(value)][i] = 1.0\n",
        "        return y_data\n",
        "\n",
        "    # Loss functions\n",
        "    def MSE_Loss(self, Y_true, y_predict):\n",
        "        MSE = np.mean((Y_true - y_predict) ** 2)\n",
        "        return MSE\n",
        "\n",
        "    def CE_loss(self, Y_true, y_predict):\n",
        "        CE = [-Y_true[i] * np.log(y_predict[i]) for i in range(len(y_predict))]\n",
        "        crossEntropy = np.mean(CE)\n",
        "        return crossEntropy\n",
        "\n",
        "    def L2R_Loss(self, weight_decay):\n",
        "        ALPHA = weight_decay\n",
        "        return ALPHA * np.sum([\n",
        "                np.linalg.norm(self.weights[str(i + 1)]) ** 2\n",
        "                for i in range(len(self.weights))\n",
        "            ]\n",
        "        )\n",
        "\n",
        "\n",
        "    def accuracy(self, Y_true, y_predict, data_size):\n",
        "        y_actual = []\n",
        "        y_predicted = []\n",
        "        ctr= 0\n",
        "        for i in range(data_size):\n",
        "            y_actual.append(np.argmax(Y_true[:, i]))\n",
        "            y_predicted.append(np.argmax(y_predict[:, i]))\n",
        "            if y_actual[i] == y_predicted[i]:\n",
        "                ctr+= 1\n",
        "        accuracy = ctr/ data_size\n",
        "        return accuracy, y_actual, y_predicted\n",
        "\n",
        "    def Xavier(self, size):\n",
        "        ip_dimension = size[1]\n",
        "        op_dimension = size[0]\n",
        "        std_dev_xavier = np.sqrt(2 / (ip_dimension + op_dimension))\n",
        "        return np.random.normal(0, std_dev_xavier, size=(op_dimension, ip_dimension))\n",
        "\n",
        "    def Random(self, size):\n",
        "        ip_dimension = size[1]\n",
        "        op_dimension = size[0]\n",
        "        return np.random.normal(0, 1, size=(op_dimension, ip_dimension))\n",
        "\n",
        "\n",
        "    def He(self,size):\n",
        "        ip_dimension = size[1]\n",
        "        op_dimension = size[0]\n",
        "        std_dev_he = np.sqrt(2 / (ip_dimension))\n",
        "        return np.random.normal(0, 1, size=(op_dimension, ip_dimension)) * std_dev_he\n",
        "\n",
        "\n",
        "    def initialize_NN(self, layers):\n",
        "        weights = {}\n",
        "        biases = {}\n",
        "        num_layers = len(layers)\n",
        "        for l in range(0, num_layers - 1):\n",
        "            W = self.initializer(size=[layers[l + 1], layers[l]])\n",
        "            b = np.zeros((layers[l + 1], 1))\n",
        "            weights[str(l + 1)] = W\n",
        "            biases[str(l + 1)] = b\n",
        "        return weights, biases\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-a9e71c1d2223>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    class Feed_Forward_NN __init__(\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    def FF_propagate(self, X_train_set, weights, biases):\n",
        "        \"\"\"\n",
        "        Returns the neural network given input data, weights, biases.\n",
        "        Arguments:\n",
        "                 : X - input matrix\n",
        "                 : Weights  - Weights matrix\n",
        "                 : biases - Bias vectors \n",
        "        \"\"\"\n",
        "        # Number of layers = length of weight matrix + 1\n",
        "        num_layers = len(weights) + 1\n",
        "        # A - Preactivations\n",
        "        # H - Activations\n",
        "        X = X_train_set\n",
        "        H = {}\n",
        "        A = {}\n",
        "        H[\"0\"] = X\n",
        "        A[\"0\"] = X\n",
        "        for l in range(0, num_layers - 2):\n",
        "            if l == 0:\n",
        "                W = weights[str(l + 1)]\n",
        "                b = biases[str(l + 1)]\n",
        "                A[str(l + 1)] = np.add(np.matmul(W, X), b)\n",
        "                H[str(l + 1)] = self.activation(A[str(l + 1)])\n",
        "            else:\n",
        "                W = weights[str(l + 1)]\n",
        "                b = biases[str(l + 1)]\n",
        "                A[str(l + 1)] = np.add(np.matmul(W, H[str(l)]), b)\n",
        "                H[str(l + 1)] = self.activation(A[str(l + 1)])\n",
        "\n",
        "        # Here the last layer is not activated as it is a regression problem\n",
        "        W = weights[str(num_layers - 1)]\n",
        "        b = biases[str(num_layers - 1)]\n",
        "        A[str(num_layers - 1)] = np.add(np.matmul(W, H[str(num_layers - 2)]), b)\n",
        "        # Y = softmax(A[-1])\n",
        "        Y = softmax(A[str(num_layers - 1)])\n",
        "        H[str(num_layers - 1)] = Y\n",
        "        return Y, H, A\n",
        "\n",
        "    def back_propagate(\n",
        "        self, Y, H, A, y_train_set, weight_decay=0\n",
        "    ):\n",
        "\n",
        "        ALPHA = weight_decay\n",
        "        gr_weights = []\n",
        "        gr_biases = []\n",
        "        num_layers = len(self.layers)\n",
        "\n",
        "        if self.loss_function == \"CROSS\":\n",
        "            globals()[\"grad_a\" + str(num_layers - 1)] = -(y_train_set - Y)\n",
        "        elif self.loss_function == \"MSE\":\n",
        "            globals()[\"grad_a\" + str(num_layers - 1)] = np.multiply(\n",
        "                2 * (Y - y_train_set), np.multiply(Y, (1 - Y))\n",
        "            )\n",
        "\n",
        "        for l in range(num_layers - 2, -1, -1):\n",
        "\n",
        "            if ALPHA != 0:\n",
        "                globals()[\"grad_W\" + str(l + 1)] = (\n",
        "                    np.outer(globals()[\"grad_a\" + str(l + 1)], H[str(l)])\n",
        "                    + ALPHA * self.weights[str(l + 1)]\n",
        "                )\n",
        "            elif ALPHA == 0:\n",
        "                globals()[\"grad_W\" + str(l + 1)] = np.outer(\n",
        "                    globals()[\"grad_a\" + str(l + 1)], H[str(l)]\n",
        "                )\n",
        "            globals()[\"grad_b\" + str(l + 1)] = globals()[\"grad_a\" + str(l + 1)]\n",
        "            gr_weights.append(globals()[\"grad_W\" + str(l + 1)])\n",
        "            gr_biases.append(globals()[\"grad_b\" + str(l + 1)])\n",
        "            if l != 0:\n",
        "                globals()[\"grad_h\" + str(l)] = np.matmul(\n",
        "                    self.weights[str(l + 1)].transpose(),\n",
        "                    globals()[\"grad_a\" + str(l + 1)],\n",
        "                )\n",
        "                globals()[\"grad_a\" + str(l)] = np.multiply(\n",
        "                    globals()[\"grad_h\" + str(l)], self.der_activation(A[str(l)])\n",
        "                )\n",
        "            elif l == 0:\n",
        "\n",
        "                globals()[\"grad_h\" + str(l)] = np.matmul(\n",
        "                    self.weights[str(l + 1)].transpose(),\n",
        "                    globals()[\"grad_a\" + str(l + 1)],\n",
        "                )\n",
        "                globals()[\"grad_a\" + str(l)] = np.multiply(\n",
        "                    globals()[\"grad_h\" + str(l)], (A[str(l)])\n",
        "                )\n",
        "        return gr_weights, gr_biases\n",
        "\n",
        "\n",
        "    def predict(self,X,len_dataset):\n",
        "        y_predict = []        \n",
        "        for i in range(len_dataset):\n",
        "\n",
        "            Y, H, A = self.FF_propagate(\n",
        "                X[:, i].reshape(self.size_imgflat, 1),\n",
        "                self.weights,\n",
        "                self.biases,\n",
        "            )\n",
        "\n",
        "            y_predict.append(Y.reshape(self.classes,))\n",
        "        y_predict = np.array(y_predict).transpose()\n",
        "        return y_predict\n",
        "\n",
        "    def sgd(self, epochs, len_dataset, learning_rate, weight_decay=0):\n",
        "        \n",
        "        training_loss = []\n",
        "        trainingaccuracy= []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "\n",
        "        X_train = self.X_train[:, :len_dataset]\n",
        "        y_train = self.y_train[:, :len_dataset]\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(len_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.size_imgflat, len_dataset)\n",
        "            y_train = y_train[:, idx].reshape(self.classes, len_dataset)\n",
        "\n",
        "            CE = []\n",
        "            dW= [\n",
        "                np.zeros((self.layers[l + 1], self.layers[l]))\n",
        "                for l in range(0, len(self.layers) - 1)\n",
        "            ]\n",
        "            db = [\n",
        "                np.zeros((self.layers[l + 1], 1))\n",
        "                for l in range(0, len(self.layers) - 1)\n",
        "            ]\n",
        "\n",
        "            for i in range(len_dataset):\n",
        "\n",
        "                Y, H, A = self.FF_propagate(\n",
        "                    X_train[:, i].reshape(self.size_imgflat, 1),\n",
        "                    self.weights,\n",
        "                    self.biases,\n",
        "                )\n",
        "                grad_weights, grad_biases = self.back_propagate(\n",
        "                    Y, H, A, y_train[:, i].reshape(self.classes, 1)\n",
        "                )\n",
        "                dW= [\n",
        "                    grad_weights[num_layers - 2 - i] for i in range(num_layers - 1)\n",
        "                ]\n",
        "                db = [\n",
        "                    grad_biases[num_layers - 2 - i] for i in range(num_layers - 1)\n",
        "                ]\n",
        "\n",
        "\n",
        "                CE.append(\n",
        "                    self.CE_loss(\n",
        "                        self.y_train[:, i].reshape(self.classes, 1), Y\n",
        "                    )\n",
        "                    + self.L2R_Loss(weight_decay)\n",
        "                )\n",
        "\n",
        "                self.weights = {\n",
        "                    str(i + 1): (self.weights[str(i + 1)] - learning_rate * dw[i])\n",
        "                    for i in range(len(self.weights))\n",
        "                }\n",
        "                self.biases = {\n",
        "                    str(i + 1): (self.biases[str(i + 1)] - learning_rate * db[i])\n",
        "                    for i in range(len(self.biases))\n",
        "                }\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            y_predict = self.predict(self.X_train, self.train_n)\n",
        "            training_loss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(y_train, y_predict, len_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.y_val, self.predict(self.X_val, self.val_n), self.val_n)[0])\n",
        "            \n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            training_loss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch, })\n",
        "        \n",
        "        return training_loss, trainingaccuracy, validationaccuracy, y_predict\n",
        "\n",
        "\n",
        "      \n",
        "    def sgd_mb(self, epochs,len_dataset, batch_size, learning_rate, weight_decay = 0):\n",
        "\n",
        "        X_train = self.X_train[:, :len_dataset]\n",
        "        y_train = self.y_train[:, :len_dataset]        \n",
        "\n",
        "        training_loss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "        npoints = 0\n",
        "\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(len_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.size_imgflat, len_dataset)\n",
        "            y_train = y_train[:, idx].reshape(self.classes, len_dataset)\n",
        "            \n",
        "            CE = []\n",
        "            \n",
        "            dW= [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            db = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "            for i in range(len_dataset):\n",
        "                \n",
        "                Y,H,A = self.FF_propagate(X_train[:,i].reshape(self.size_imgflat,1), self.weights, self.biases) \n",
        "                grad_weights, grad_biases = self.back_propagate(Y,H,A,y_train[:,i].reshape(self.classes,1))\n",
        "                \n",
        "                dW= [grad_weights[num_layers-2 - i] + dw[i] for i in range(num_layers - 1)]\n",
        "                db = [grad_biases[num_layers-2 - i] + db[i] for i in range(num_layers - 1)]\n",
        "                \n",
        "                #y_predict.append(Y.reshape(self.classes,))\n",
        "                CE.append(self.CE_loss(self.y_train[:,i].reshape(self.classes,1), Y) + self.L2R_Loss(weight_decay))\n",
        "                \n",
        "                npoints +=1\n",
        "                \n",
        "                if int(npoints) % batch_size == 0:\n",
        "                    \n",
        "                    \n",
        "                    self.weights = {str(i+1):(self.weights[str(i+1)] - learning_rate*dw[i]/batch_size) for i in range(len(self.weights))} \n",
        "                    self.biases = {str(i+1):(self.biases[str(i+1)] - learning_rate*db[i]) for i in range(len(self.biases))}\n",
        "                    \n",
        "                    dW= [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    db = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "            y_predict = self.predict(self.X_train, self.train_n)\n",
        "            training_loss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(y_train, y_predict, len_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.y_val, self.predict(self.X_val, self.val_n), self.val_n)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            training_loss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "                    \n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "            \n",
        "        return training_loss, trainingaccuracy, validationaccuracy, y_predict\n",
        "\n",
        "\n",
        "\n",
        "    def mgd(self, epochs,len_dataset, batch_size, learning_rate, weight_decay = 0):\n",
        "        GAMMA = 0.9\n",
        "\n",
        "        X_train = self.X_train[:, :len_dataset]\n",
        "        y_train = self.y_train[:, :len_dataset]        \n",
        "\n",
        "        \n",
        "        training_loss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "        prev_vw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        prev_vb = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        npoints = 0\n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(len_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.size_imgflat, len_dataset)\n",
        "            y_train = y_train[:, idx].reshape(self.classes, len_dataset)\n",
        "\n",
        "            CE = []\n",
        "            #y_predict = []\n",
        "            dW= [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            db = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "            \n",
        "\n",
        "            for i in range(len_dataset):\n",
        "                Y,H,A = self.FF_propagate(self.X_train[:,i].reshape(self.size_imgflat,1), self.weights, self.biases) \n",
        "                grad_weights, grad_biases = self.back_propagate(Y,H,A,self.y_train[:,i].reshape(self.classes,1))\n",
        "                \n",
        "                dW= [grad_weights[num_layers-2 - i] + dw[i] for i in range(num_layers - 1)]\n",
        "                db = [grad_biases[num_layers-2 - i] + db[i] for i in range(num_layers - 1)]\n",
        "\n",
        "                CE.append(self.CE_loss(self.y_train[:,i].reshape(self.classes,1), Y) + self.L2R_Loss(weight_decay))\n",
        "                \n",
        "                npoints +=1\n",
        "                \n",
        "                if int(npoints) % batch_size == 0:\n",
        "\n",
        "                    vw = [GAMMA*prev_vw[i] + learning_rate*dw[i]/batch_size for i in range(num_layers - 1)]\n",
        "                    vb = [GAMMA*prev_vb[i] + learning_rate*db[i]/batch_size for i in range(num_layers - 1)]\n",
        "                    \n",
        "                    self.weights = {str(i+1) : (self.weights[str(i+1)] - vw[i]) for i in range(len(self.weights))}\n",
        "                    self.biases = {str(i+1): (self.biases[str(i+1)] - vb[i]) for i in range(len(self.biases))}\n",
        "\n",
        "                    prev_vw = vw\n",
        "                    prev_vb = vb\n",
        "\n",
        "                    #resetting gradient updates\n",
        "                    dW= [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    db = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            y_predict = self.predict(self.X_train, self.train_n)\n",
        "            training_loss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(y_train, y_predict, len_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.y_val, self.predict(self.X_val, self.val_n), self.val_n)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            training_loss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "\n",
        "\n",
        "        return training_loss, trainingaccuracy, validationaccuracy, y_predict\n",
        "\n",
        "\n",
        " \n",
        " \n",
        "    def stochastic_ng(self,epochs,len_dataset, learning_rate, weight_decay = 0):\n",
        "        GAMMA = 0.9\n",
        "\n",
        "        X_train = self.X_train[:, :len_dataset]\n",
        "        y_train = self.y_train[:, :len_dataset]        \n",
        "\n",
        "        training_loss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "        \n",
        "        prev_vw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        prev_vb = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(len_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.size_imgflat, len_dataset)\n",
        "            y_train = y_train[:, idx].reshape(self.classes, len_dataset)\n",
        "\n",
        "            CE = []\n",
        "            #y_predict = []  \n",
        "            \n",
        "            dW= [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            db = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "            \n",
        "            vw = [GAMMA*prev_vw[i] for i in range(0, len(self.layers)-1)]  \n",
        "            vb = [GAMMA*prev_vb[i] for i in range(0, len(self.layers)-1)]\n",
        "                        \n",
        "            for i in range(len_dataset):\n",
        "                w_inter = {str(i+1) : self.weights[str(i+1)] - vw[i] for i in range(0, len(self.layers)-1)}\n",
        "                b_inter = {str(i+1) : self.biases[str(i+1)] - vb[i] for i in range(0, len(self.layers)-1)}\n",
        "                \n",
        "                Y,H,A = self.FF_propagate(self.X_train[:,i].reshape(self.size_imgflat,1), w_inter, b_inter) \n",
        "                grad_weights, grad_biases = self.back_propagate(Y,H,A,self.y_train[:,i].reshape(self.classes,1))\n",
        "                \n",
        "                dW= [grad_weights[num_layers-2 - i] for i in range(num_layers - 1)]\n",
        "                db = [grad_biases[num_layers-2 - i] for i in range(num_layers - 1)]\n",
        "\n",
        "                CE.append(self.CE_loss(self.y_train[:,i].reshape(self.classes,1), Y) + self.L2R_Loss(weight_decay))\n",
        "                            \n",
        "                vw = [GAMMA*prev_vw[i] + learning_rate*dw[i] for i in range(num_layers - 1)]\n",
        "                vb = [GAMMA*prev_vb[i] + learning_rate*db[i] for i in range(num_layers - 1)]\n",
        "        \n",
        "                self.weights = {str(i+1):self.weights[str(i+1)] - vw[i] for i in range(len(self.weights))} \n",
        "                self.biases = {str(i+1):self.biases[str(i+1)] - vb[i] for i in range(len(self.biases))}\n",
        "                \n",
        "                prev_vw = vw\n",
        "                prev_vb = vb\n",
        "    \n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "            #y_predict = np.array(y_predict).transpose()\n",
        "            y_predict = self.predict(self.X_train, self.train_n)\n",
        "            training_loss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(y_train, y_predict, len_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.y_val, self.predict(self.X_val, self.val_n), self.val_n)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            training_loss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "                    \n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "        \n",
        "        return training_loss, trainingaccuracy, validationaccuracy, y_predict\n",
        "    \n",
        "\n",
        "    def Ng(self,epochs,len_dataset, batch_size,learning_rate, weight_decay = 0):\n",
        "        GAMMA = 0.9\n",
        "\n",
        "        X_train = self.X_train[:, :len_dataset]\n",
        "        y_train = self.y_train[:, :len_dataset]        \n",
        "\n",
        "\n",
        "        training_loss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "        \n",
        "        prev_vw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        prev_vb = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        npoints = 0\n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(len_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.size_imgflat, len_dataset)\n",
        "            y_train = y_train[:, idx].reshape(self.classes, len_dataset)\n",
        "\n",
        "            CE = []\n",
        "            \n",
        "            dW= [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            db = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "            \n",
        "            vw = [GAMMA*prev_vw[i] for i in range(0, len(self.layers)-1)]  \n",
        "            vb = [GAMMA*prev_vb[i] for i in range(0, len(self.layers)-1)]\n",
        "\n",
        "            for i in range(len_dataset):\n",
        "                w_inter = {str(i+1) : self.weights[str(i+1)] - vw[i] for i in range(0, len(self.layers)-1)}\n",
        "                b_inter = {str(i+1) : self.biases[str(i+1)] - vb[i] for i in range(0, len(self.layers)-1)}\n",
        "                \n",
        "                Y,H,A = self.FF_propagate(self.X_train[:,i].reshape(self.size_imgflat,1), w_inter, b_inter) \n",
        "                grad_weights, grad_biases = self.back_propagate(Y,H,A,self.y_train[:,i].reshape(self.classes,1))\n",
        "                \n",
        "                dW= [grad_weights[num_layers-2 - i] + dw[i] for i in range(num_layers - 1)]\n",
        "                db = [grad_biases[num_layers-2 - i] + db[i] for i in range(num_layers - 1)]\n",
        "\n",
        "                #y_predict.append(Y.reshape(self.classes,))\n",
        "                CE.append(self.CE_loss(self.y_train[:,i].reshape(self.classes,1), Y) + self.L2R_Loss(weight_decay))\n",
        "\n",
        "                npoints +=1\n",
        "                \n",
        "                if int(npoints) % batch_size == 0:                            \n",
        "\n",
        "                    vw = [GAMMA*prev_vw[i] + learning_rate*dw[i]/batch_size for i in range(num_layers - 1)]\n",
        "                    vb = [GAMMA*prev_vb[i] + learning_rate*db[i]/batch_size for i in range(num_layers - 1)]\n",
        "        \n",
        "                    self.weights ={str(i+1):self.weights[str(i+1)]  - vw[i] for i in range(len(self.weights))}\n",
        "                    self.biases = {str(i+1):self.biases[str(i+1)]  - vb[i] for i in range(len(self.biases))}\n",
        "                \n",
        "                    prev_vw = vw\n",
        "                    prev_vb = vb\n",
        "\n",
        "                    dW= [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    db = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "    \n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "\n",
        "            y_predict = self.predict(self.X_train, self.train_n)\n",
        "            training_loss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(y_train, y_predict, len_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.y_val, self.predict(self.X_val, self.val_n), self.val_n)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            training_loss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "        \n",
        "        return training_loss, trainingaccuracy, validationaccuracy, y_predict\n",
        "    \n",
        "\n",
        "    \n",
        "    def rmsProp(self, epochs,len_dataset, batch_size, learning_rate, weight_decay = 0):\n",
        "\n",
        "\n",
        "        X_train = self.X_train[:, :len_dataset]\n",
        "        y_train = self.y_train[:, :len_dataset]        \n",
        "\n",
        "        \n",
        "        training_loss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "        EPS, BETA = 1e-8, 0.9\n",
        "        \n",
        "        vw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        vb = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        npoints = 0        \n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(len_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.size_imgflat, len_dataset)\n",
        "            y_train = y_train[:, idx].reshape(self.classes, len_dataset)\n",
        "\n",
        "\n",
        "            CE = []\n",
        "            #y_predict = []\n",
        "                        \n",
        "            dW= [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            db = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "            for i in range(len_dataset):\n",
        "            \n",
        "                Y,H,A = self.FF_propagate(self.X_train[:,i].reshape(self.size_imgflat,1), self.weights, self.biases) \n",
        "                grad_weights, grad_biases = self.back_propagate(Y,H,A,self.y_train[:,i].reshape(self.classes,1))\n",
        "            \n",
        "                dW= [grad_weights[num_layers-2 - i] + dw[i] for i in range(num_layers - 1)]\n",
        "                db = [grad_biases[num_layers-2 - i] + db[i] for i in range(num_layers - 1)]\n",
        "                \n",
        "                 CE.append(self.CE_loss(self.y_train[:,i].reshape(self.classes,1), Y) + self.L2R_Loss(weight_decay))            \n",
        "                npoints +=1\n",
        "                \n",
        "                if int(npoints) % batch_size == 0:\n",
        "                \n",
        "                    vw = [BETA*vw[i] + (1-BETA)*(dw[i])**2 for i in range(num_layers - 1)]\n",
        "                    vb = [BETA*vb[i] + (1-BETA)*(db[i])**2 for i in range(num_layers - 1)]\n",
        "\n",
        "                    self.weights = {str(i+1):self.weights[str(i+1)]  - dw[i]*(learning_rate/np.sqrt(vw[i]+EPS)) for i in range(len(self.weights))} \n",
        "                    self.biases = {str(i+1):self.biases[str(i+1)]  - db[i]*(learning_rate/np.sqrt(vb[i]+EPS)) for i in range(len(self.biases))}\n",
        "\n",
        "                    dW= [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    db = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "    \n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "            #y_predict = np.array(y_predict).transpose()\n",
        "            y_predict = self.predict(self.X_train, self.train_n)\n",
        "            training_loss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(y_train, y_predict, len_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.y_val, self.predict(self.X_val, self.val_n), self.val_n)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            training_loss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "                    \n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "        \n",
        "        return training_loss, trainingaccuracy, validationaccuracy, y_predict  \n",
        "\n",
        "\n",
        "\n",
        "    def adam(self, epochs,len_dataset, batch_size, learning_rate, weight_decay = 0):\n",
        "        \n",
        "        X_train = self.X_train[:, :len_dataset]\n",
        "        y_train = self.y_train[:, :len_dataset]        \n",
        "\n",
        "        training_loss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        num_layers = len(self.layers)\n",
        "        EPS, BETA1, BETA2 = 1e-8, 0.9, 0.99\n",
        "        \n",
        "       mw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "       mb = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        vw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        vb = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]        \n",
        "        \n",
        "       mw_hat = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "       mb_hat = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        vw_hat = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        vb_hat = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]   \n",
        "        \n",
        "        npoints = 0 \n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(len_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.size_imgflat, len_dataset)\n",
        "            y_train = y_train[:, idx].reshape(self.classes, len_dataset)\n",
        "\n",
        "\n",
        "            CE = []\n",
        "            \n",
        "            dW= [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            db = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "            \n",
        "           \n",
        "            for i in range(len_dataset):\n",
        "                Y,H,A = self.FF_propagate(self.X_train[:,i].reshape(self.size_imgflat,1), self.weights, self.biases) \n",
        "                grad_weights, grad_biases = self.back_propagate(Y,H,A,self.y_train[:,i].reshape(self.classes,1))\n",
        "                \n",
        "                dW= [grad_weights[num_layers-2 - i] + dw[i] for i in range(num_layers - 1)]\n",
        "                db = [grad_biases[num_layers-2 - i] + db[i] for i in range(num_layers - 1)]\n",
        "\n",
        "                CE.append(self.CE_loss(self.y_train[:,i].reshape(self.classes,1), Y) + self.L2R_Loss(weight_decay))                 \n",
        "\n",
        "                npoints += 1\n",
        "                ctr= 0\n",
        "                if int(npoints) % batch_size == 0:\n",
        "                    ctr+= 1\n",
        "                \n",
        "                   mw = [BETA1*m_w[i] + (1-BETA1)*dw[i] for i in range(num_layers - 1)]\n",
        "                   mb = [BETA1*m_b[i] + (1-BETA1)*db[i] for i in range(num_layers - 1)]\n",
        "                \n",
        "                    vw = [BETA2*vw[i] + (1-BETA2)*(dw[i])**2 for i in range(num_layers - 1)]\n",
        "                    vb = [BETA2*vb[i] + (1-BETA2)*(db[i])**2 for i in range(num_layers - 1)]\n",
        "                    \n",
        "                   mw_hat = [m_w[i]/(1-BETA1**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                   mb_hat = [m_b[i]/(1-BETA1**(epoch+1)) for i in range(num_layers - 1)]            \n",
        "                \n",
        "                    vw_hat = [vw[i]/(1-BETA2**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                    vb_hat = [vb[i]/(1-BETA2**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                \n",
        "                    self.weights = {str(i+1):self.weights[str(i+1)] - (learning_rate/np.sqrt(vw[i]+EPS))*m_w_hat[i] for i in range(len(self.weights))} \n",
        "                    self.biases = {str(i+1):self.biases[str(i+1)] - (learning_rate/np.sqrt(vb[i]+EPS))*m_b_hat[i] for i in range(len(self.biases))}\n",
        "\n",
        "                    dW= [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    db = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            #y_predict = np.array(y_predict).transpose()\n",
        "            y_predict = self.predict(self.X_train, self.train_n)\n",
        "            training_loss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(y_train, y_predict, len_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.y_val, self.predict(self.X_val, self.val_n), self.val_n)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            training_loss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "                    \n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "        \n",
        "        return training_loss, trainingaccuracy, validationaccuracy, y_predict\n",
        "\n",
        "\n",
        "    \n",
        "    def nadam(self, epochs,len_dataset, batch_size, learning_rate, weight_decay = 0):\n",
        "\n",
        "        X_train = self.X_train[:, :len_dataset]\n",
        "        y_train = self.y_train[:, :len_dataset]        \n",
        "\n",
        "        \n",
        "        training_loss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        num_layers = len(self.layers)\n",
        "        \n",
        "        GAMMA, EPS, BETA1, BETA2 = 0.9, 1e-8, 0.9, 0.99\n",
        "\n",
        "       mw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "       mb = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        vw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        vb = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]        \n",
        "\n",
        "       mw_hat = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "       mb_hat = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        vw_hat = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        vb_hat = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)] \n",
        "\n",
        "        npoints = 0 \n",
        "        \n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(len_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.size_imgflat, len_dataset)\n",
        "            y_train = y_train[:, idx].reshape(self.classes, len_dataset)\n",
        "\n",
        "            CE = []\n",
        "            #y_predict = []\n",
        "\n",
        "            dW= [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            db = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "            for i in range(len_dataset):\n",
        "\n",
        "                Y,H,A = self.FF_propagate(self.X_train[:,i].reshape(self.size_imgflat,1), self.weights, self.biases) \n",
        "                grad_weights, grad_biases = self.back_propagate(Y,H,A,self.y_train[:,i].reshape(self.classes,1))\n",
        "\n",
        "                dW= [grad_weights[num_layers-2 - i] + dw[i] for i in range(num_layers - 1)]\n",
        "                db = [grad_biases[num_layers-2 - i] + db[i] for i in range(num_layers - 1)]\n",
        "\n",
        "                #y_predict.append(Y.reshape(self.classes,))\n",
        "                CE.append(self.CE_loss(self.y_train[:,i].reshape(self.classes,1), Y) + self.L2R_Loss(weight_decay))   \n",
        "                npoints += 1\n",
        "                \n",
        "                if npoints % batch_size == 0:\n",
        "                    \n",
        "                   mw = [BETA1*m_w[i] + (1-BETA1)*dw[i] for i in range(num_layers - 1)]\n",
        "                   mb = [BETA1*m_b[i] + (1-BETA1)*db[i] for i in range(num_layers - 1)]\n",
        "                    \n",
        "                    vw = [BETA2*vw[i] + (1-BETA2)*(dw[i])**2 for i in range(num_layers - 1)]\n",
        "                    vb = [BETA2*vb[i] + (1-BETA2)*(db[i])**2 for i in range(num_layers - 1)]\n",
        "                    \n",
        "                   mw_hat = [m_w[i]/(1-BETA1**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                   mb_hat = [m_b[i]/(1-BETA1**(epoch+1)) for i in range(num_layers - 1)]            \n",
        "                    \n",
        "                    vw_hat = [vw[i]/(1-BETA2**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                    vb_hat = [vb[i]/(1-BETA2**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                    \n",
        "                    self.weights = {str(i+1):self.weights[str(i+1)] - (learning_rate/(np.sqrt(vw_hat[i])+EPS))*(BETA1*m_w_hat[i]+ (1-BETA1)*dw[i]) for i in range(len(self.weights))} \n",
        "                    self.biases = {str(i+1):self.biases[str(i+1)] - (learning_rate/(np.sqrt(vb_hat[i])+EPS))*(BETA1*m_b_hat[i] + (1-BETA1)*db[i]) for i in range(len(self.biases))}\n",
        "\n",
        "                    dW= [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    db = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "             \n",
        "            elapsed = time.time() - start_time\n",
        "\n",
        "            #y_predict = np.array(y_predict).transpose()\n",
        "            y_predict = self.predict(self.X_train, self.train_n)\n",
        "            training_loss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(y_train, y_predict, len_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.y_val, self.predict(self.X_val, self.val_n), self.val_n)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            training_loss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "            \n",
        "        return training_loss, trainingaccuracy, validationaccuracy, y_predict  \n"
      ],
      "metadata": {
        "id": "n99Ta1Hq52cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_3tVRXJA8Rv"
      },
      "source": [
        "\n",
        "#Sweep\n",
        "\n",
        "(trainIn, trainOut), (test_ip, test_op) = fashion_mnist.load_data()\n",
        "\n",
        "train_n_full = trainOut.shape[0]\n",
        "train_n = int(0.9*train_n_full)\n",
        "n_validation = int(0.1 * trainOut.shape[0])\n",
        "N_test = test_op.shape[0]\n",
        "\n",
        "\n",
        "idx  = np.random.choice(trainOut.shape[0], train_n_full, replace=False)\n",
        "idx2 = np.random.choice(test_op.shape[0], N_test, replace=False)\n",
        "\n",
        "trainInFull = trainIn[idx, :]\n",
        "trainOutFull = trainOut[idx]\n",
        "\n",
        "trainIn = trainInFull[:train_n,:]\n",
        "trainOut = trainOutFull[:train_n]\n",
        "\n",
        "validIn = trainInFull[train_n:, :]\n",
        "validOut = trainOutFull[train_n:]    \n",
        "\n",
        "test_ip = test_ip[idx2, :]\n",
        "test_op = test_op[idx2]\n",
        "\n",
        "\n",
        "sweep_config = {\n",
        "  \"name\": \"Bayesian Sweep\",\n",
        "  \"method\": \"bayes\",\n",
        "  \"metric\":{\n",
        "  \"name\": \"validationaccuracy\",\n",
        "  \"goal\": \"maximize\"\n",
        "  },\n",
        "  \"parameters\": {\n",
        "        \"maxNo_epoch\": {\n",
        "            \"values\": [5, 10]\n",
        "        },\n",
        "\n",
        "        \"initializer\": {\n",
        "            \"values\": [\"RANDOM\", \"XAVIER\", \"HE\"]\n",
        "        },\n",
        "\n",
        "        \"num_layers\": {\n",
        "            \"values\": [2, 3, 4]\n",
        "        },\n",
        "        \n",
        "        \n",
        "        \"hidden_neurons\": {\n",
        "            \"values\": [32, 64, 128]\n",
        "        },\n",
        "        \n",
        "        \"activation\": {\n",
        "            \"values\": [ 'TANH',  'SIGMOID', 'RELU']\n",
        "        },\n",
        "        \n",
        "        \"learning_rate\": {\n",
        "            \"values\": [0.001, 0.0001]\n",
        "        },\n",
        "        \n",
        "        \n",
        "        \"weight_decay\": {\n",
        "            \"values\": [0, 0.0005,0.5]\n",
        "        },\n",
        "        \n",
        "        \"optimizer\": {\n",
        "            \"values\": [\"SGD\", \"MGD\", \"Ng\", \"RMSPROP\", \"ADAM\",\"NADAM\"]\n",
        "        },\n",
        "                    \n",
        "        \"batch_size\": {\n",
        "            \"values\": [16, 32, 64]\n",
        "        }\n",
        "        \n",
        "        \n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config,project='assi_1_q_4', entity='gadhapremdas2017')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyC4fPR8Cwgp"
      },
      "source": [
        "def train():    \n",
        "    config_defaults = dict(\n",
        "            max_epochs=10,\n",
        "            num_hidden_layers=3,\n",
        "            num_hidden_neurons=128,\n",
        "            weight_decay=0,\n",
        "            learning_rate=1e-3,\n",
        "            optimizer=\"NADAM\",\n",
        "            batch_size=32,\n",
        "            activation=\"SIGMOID\",\n",
        "            initializer=\"XAVIER\",\n",
        "            loss=\"MSE\",\n",
        "        )\n",
        "        \n",
        "    #wandb.init(config = config_defaults)\n",
        "    wandb.init(project='assi_1_mse', entity='gadhapremdas2017',config = config_defaults)\n",
        "\n",
        "\n",
        "    wandb.run.name = \"MSE_hl_\" + str(wandb.config.num_hidden_layers) + \"_hn_\" + str(wandb.config.num_hidden_neurons) + \"_opt_\" + wandb.config.optimizer + \"_act_\" + wandb.config.activation + \"_lr_\" + str(wandb.config.learning_rate) + \"_bs_\"+str(wandb.config.batch_size) + \"_init_\" + wandb.config.initializer + \"_ep_\"+ str(wandb.config.max_epochs)+ \"_l2_\" + str(wandb.config.weight_decay) \n",
        "    CONFIG = wandb.config\n",
        "\n",
        "    # sweep_id = wandb.sweep(sweep_config)\n",
        "  \n",
        "\n",
        "    FFNN = Feed_Forward_NN(\n",
        "        hidden_layers=CONFIG.hidden_layers,\n",
        "        hidden_neurons=CONFIG.hidden_neurons,\n",
        "        Xtrain_ini=trainIn,\n",
        "        ytrain_ini=trainOut,\n",
        "        train_n = train_n,\n",
        "        Xval_ini = validIn,\n",
        "        yval_ini = validOut,\n",
        "        val_n = n_validation,\n",
        "        Xtest_ini = test_ip,\n",
        "        ytest_ini = test_op,\n",
        "        N_test = N_test,\n",
        "        optimizer = CONFIG.optimizer,\n",
        "        batch_size = CONFIG.batch_size,\n",
        "        weight_decay = CONFIG.weight_decay,\n",
        "        learning_rate = CONFIG.learning_rate,\n",
        "        maxNo_epoch = CONFIG.maxNo_epoch,\n",
        "        activation = CONFIG.activation,\n",
        "        initializer = CONFIG.initializer,\n",
        "        loss = CONFIG.loss\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    training_loss, trainingaccuracy, validationaccuracy, y_predict_train = FFNN.optimizer(FFNN.maxNo_epoch, FFNN.train_n, FFNN.batch_size, FFNN.learning_rate)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z4EENrgtDTnu",
        "outputId": "959326b2-e230-49d8-ae06-e0aa0b1d4067"
      },
      "source": [
        "wandb.agent(sweep_id, train, count = 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2p0u7imu with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: TANH\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitializer: XAVIER\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden_neurons: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: NAG\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgadhapremdas2017\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230318_041229-2p0u7imu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gadhapremdas2017/CS6910-DeepLearningFundamentals-Assignment1/runs/2p0u7imu' target=\"_blank\">sleek-sweep-1</a></strong> to <a href='https://wandb.ai/gadhapremdas2017/CS6910-DeepLearningFundamentals-Assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/gadhapremdas2017/CS6910-DeepLearningFundamentals-Assignment1/sweeps/qw7cq8pb' target=\"_blank\">https://wandb.ai/gadhapremdas2017/CS6910-DeepLearningFundamentals-Assignment1/sweeps/qw7cq8pb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gadhapremdas2017/CS6910-DeepLearningFundamentals-Assignment1' target=\"_blank\">https://wandb.ai/gadhapremdas2017/CS6910-DeepLearningFundamentals-Assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/gadhapremdas2017/CS6910-DeepLearningFundamentals-Assignment1/sweeps/qw7cq8pb' target=\"_blank\">https://wandb.ai/gadhapremdas2017/CS6910-DeepLearningFundamentals-Assignment1/sweeps/qw7cq8pb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gadhapremdas2017/CS6910-DeepLearningFundamentals-Assignment1/runs/2p0u7imu' target=\"_blank\">https://wandb.ai/gadhapremdas2017/CS6910-DeepLearningFundamentals-Assignment1/runs/2p0u7imu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 8.905e-02, Training accuracy:0.80, Validation Accuracy: 0.81, Time: 64.76, Learning Rate: 1.000e-03\n",
            "Epoch: 1, Loss: 5.210e-02, Training accuracy:0.83, Validation Accuracy: 0.83, Time: 43.08, Learning Rate: 1.000e-03\n",
            "Epoch: 2, Loss: 4.596e-02, Training accuracy:0.85, Validation Accuracy: 0.85, Time: 42.99, Learning Rate: 1.000e-03\n",
            "Epoch: 3, Loss: 4.274e-02, Training accuracy:0.85, Validation Accuracy: 0.86, Time: 43.98, Learning Rate: 1.000e-03\n",
            "Epoch: 4, Loss: 4.055e-02, Training accuracy:0.86, Validation Accuracy: 0.86, Time: 43.00, Learning Rate: 1.000e-03\n",
            "Epoch: 5, Loss: 3.890e-02, Training accuracy:0.87, Validation Accuracy: 0.87, Time: 51.23, Learning Rate: 1.000e-03\n",
            "Epoch: 6, Loss: 3.757e-02, Training accuracy:0.87, Validation Accuracy: 0.87, Time: 43.28, Learning Rate: 1.000e-03\n",
            "Epoch: 7, Loss: 3.645e-02, Training accuracy:0.87, Validation Accuracy: 0.87, Time: 43.15, Learning Rate: 1.000e-03\n",
            "Epoch: 8, Loss: 3.549e-02, Training accuracy:0.88, Validation Accuracy: 0.87, Time: 43.12, Learning Rate: 1.000e-03\n",
            "Epoch: 9, Loss: 3.465e-02, Training accuracy:0.88, Validation Accuracy: 0.87, Time: 42.97, Learning Rate: 1.000e-03\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▃▂▂▂▂▁▁▁▁</td></tr><tr><td>trainingaccuracy</td><td>▁▄▅▆▆▇▇▇██</td></tr><tr><td>validationaccuracy</td><td>▁▄▅▆▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.03465</td></tr><tr><td>trainingaccuracy</td><td>0.87991</td></tr><tr><td>validationaccuracy</td><td>0.87367</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sleek-sweep-1</strong> at: <a href='https://wandb.ai/gadhapremdas2017/CS6910-DeepLearningFundamentals-Assignment1/runs/2p0u7imu' target=\"_blank\">https://wandb.ai/gadhapremdas2017/CS6910-DeepLearningFundamentals-Assignment1/runs/2p0u7imu</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230318_041229-2p0u7imu/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: z65g7xx8 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: SIGMOID\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitializer: HE\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden_neurons: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: MGD\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230318_042131-z65g7xx8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gadhapremdas2017/CS6910-DeepLearningFundamentals-Assignment1/runs/z65g7xx8' target=\"_blank\">dulcet-sweep-2</a></strong> to <a href='https://wandb.ai/gadhapremdas2017/CS6910-DeepLearningFundamentals-Assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/gadhapremdas2017/CS6910-DeepLearningFundamentals-Assignment1/sweeps/qw7cq8pb' target=\"_blank\">https://wandb.ai/gadhapremdas2017/CS6910-DeepLearningFundamentals-Assignment1/sweeps/qw7cq8pb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gadhapremdas2017/CS6910-DeepLearningFundamentals-Assignment1' target=\"_blank\">https://wandb.ai/gadhapremdas2017/CS6910-DeepLearningFundamentals-Assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/gadhapremdas2017/CS6910-DeepLearningFundamentals-Assignment1/sweeps/qw7cq8pb' target=\"_blank\">https://wandb.ai/gadhapremdas2017/CS6910-DeepLearningFundamentals-Assignment1/sweeps/qw7cq8pb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gadhapremdas2017/CS6910-DeepLearningFundamentals-Assignment1/runs/z65g7xx8' target=\"_blank\">https://wandb.ai/gadhapremdas2017/CS6910-DeepLearningFundamentals-Assignment1/runs/z65g7xx8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 2.359e-01, Training accuracy:0.07, Validation Accuracy: 0.07, Time: 44.50, Learning Rate: 1.000e-04\n",
            "Epoch: 1, Loss: 2.289e-01, Training accuracy:0.29, Validation Accuracy: 0.28, Time: 45.19, Learning Rate: 1.000e-04\n",
            "Epoch: 2, Loss: 2.278e-01, Training accuracy:0.37, Validation Accuracy: 0.37, Time: 44.14, Learning Rate: 1.000e-04\n",
            "Epoch: 3, Loss: 2.264e-01, Training accuracy:0.39, Validation Accuracy: 0.39, Time: 44.46, Learning Rate: 1.000e-04\n",
            "Epoch: 4, Loss: 2.247e-01, Training accuracy:0.39, Validation Accuracy: 0.40, Time: 47.70, Learning Rate: 1.000e-04\n",
            "Epoch: 5, Loss: 2.224e-01, Training accuracy:0.38, Validation Accuracy: 0.39, Time: 49.16, Learning Rate: 1.000e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuiqRBuDUhIw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6de12656-7ca0-4405-ec98-721c96c25eb6"
      },
      "source": [
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
          ]
        }
      ]
    }
  ]
}